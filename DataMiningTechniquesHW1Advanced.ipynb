{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoelRotti/ADStructures/blob/master/DataMiningTechniquesHW1Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aYAQ9Gg-t0q"
      },
      "source": [
        "## Summarizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "PcMT7nWJ4YKn",
        "outputId": "4ec7d3c2-6afb-4b8c-b513-d56e2997cf3f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import data_table\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"dataset_mood_smartphone.csv\", parse_dates=[\"time\"])\n",
        "df = df.rename(columns = {\"Unnamed: 0\" : \"index\", \"id\" : \"user_id\", \"time\" : \"datetime\"}, \n",
        "               inplace = False)\n",
        "# print(df[df['user_id']=='AS14.01']) # all information for one user Id\n",
        "print(df['user_id'].value_counts()) # instances per user\n",
        "print(\"Number of different user id's:\", len(df['user_id'].value_counts())) # number of users\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25b2a001914b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset_mood_smartphone.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m df = df.rename(columns = {\"Unnamed: 0\" : \"index\", \"id\" : \"user_id\", \"time\" : \"datetime\"}, \n\u001b[1;32m     13\u001b[0m                inplace = False)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_mood_smartphone.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nHlZfhcHWn"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkqGESdeA8YK"
      },
      "source": [
        "# unstack makes seperated columns for all different variable values\n",
        "df_seperated = df.set_index(['variable','index', 'user_id', 'datetime']).unstack(['variable'])\n",
        "# Seperate columns for everything\n",
        "df_seperated.columns = ['activity',\t'appCat.builtin',\n",
        "                        'appCat.communication', 'appCat.entertainment',\t'appCat.finance',\n",
        "                        'appCat.game', 'appCat.office', 'appCat.other', 'appCat.social',\n",
        "                        'appCat.travel', 'appCat.unknown', 'appCat.utilities', \n",
        "                        'appCat.weather', 'call', 'circumplex.arousal', 'circumplex.valence',\n",
        "                        'mood', 'screen', 'sms']\n",
        "\n",
        "# These resampler functions can aggregate (by summing or averaging) data over periods 'D'\n",
        "# here means that this function aggregates over days (because datetime is in DayTime format)\n",
        "\n",
        "def resampler_sum(x):    \n",
        "    return x.set_index('datetime').resample('D').sum()\n",
        "\n",
        "def resampler_avg(x):\n",
        "    return x.set_index('datetime').resample('D').mean()\n",
        "    \n",
        "# apply the resamplers\n",
        "df_aggregated_daily = df_seperated.copy().reset_index(level=2).groupby(level=1).apply(resampler_sum)\n",
        "df_aggregated_avg = df_seperated.copy().reset_index(level=2).groupby(level=1).apply(resampler_avg)\n",
        "\n",
        "# These values are useful as averages (not summed!) \n",
        "df_aggregated_daily[\"mood\"] = df_aggregated_avg[\"mood\"]\n",
        "df_aggregated_daily[\"circumplex.arousal\"] = df_aggregated_avg[\"circumplex.arousal\"]\n",
        "df_aggregated_daily[\"circumplex.valence\"] = df_aggregated_avg[\"circumplex.valence\"]\n",
        "df_aggregated_daily[\"activity\"] = df_aggregated_avg[\"activity\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkaId_qYy66B"
      },
      "source": [
        "## Handling missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT2tRehXy5gW"
      },
      "source": [
        "df_including_na = df_aggregated_daily.copy()\n",
        "\n",
        "# when there is no value for mood the instance is useless.\n",
        "df_including_na.dropna(subset = [\"mood\"], inplace= True)\n",
        "\n",
        "# all different user\n",
        "persons = df_including_na.index.get_level_values(0).unique()\n",
        "\n",
        "# Shows which days are missing\n",
        "for person in persons:\n",
        "    # print(person)\n",
        "    df_per_personX = df_including_na.xs(person, level='user_id')\n",
        "    # print(\"First recorded date: \", df_per_personX.index.min())\n",
        "    # print(\"Final date: \", df_per_personX.index.max())\n",
        "    # print(\"Duration: \", df_per_personX.index.max()-df_per_personX.index.min())\n",
        "    # print(pd.date_range(start = df_per_personX.index.min(), end = df_per_personX.index.max()).difference(df_per_personX.index))\n",
        "\n",
        "## Shows dataframe per user to observe where the missing days are\n",
        "# df_including_na.xs('AS14.01', level='user_id') #--> participant started later (so delete first two days for which there are some measurements)\n",
        "# df_including_na.xs('AS14.03', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.06', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.12', level='user_id') #--> participant started later (so delete first day for which there are some measurements)\n",
        "# df_including_na.xs('AS14.14', level='user_id') #--> 3 randomly missing days (interpolate)\n",
        "# df_including_na.xs('AS14.15', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.16', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.17', level='user_id') #--> skipped a whole week after a week of missing activity values. SO WHAT TO DO HERE?? <-------------------------------------------------\n",
        "# df_including_na.xs('AS14.23', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.24', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.25', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.26', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.27', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.28', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.15', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.29', level='user_id') #--> 2 days randomly missing, not sequentially (interpolate)\n",
        "# df_including_na.xs('AS14.31', level='user_id') #--> just one day missing (interpolate)\n",
        "# df_including_na.xs('AS14.32', level='user_id') #--> 5 days randomly missing of which 2 follow each other directly (interpolate)\n",
        "# df_including_na.xs('AS14.33', level='user_id') #--> 3 days randomly missing of which 2 follow each other directly (interpolate)\n",
        "\n",
        "## EVERYONE MISSES THE 6TH OF MAY -->> mention this in report\n",
        "\n",
        "# Here the rows are deleted manually where people had some trouble in the beginning of the \n",
        "# trial (where no phone usage information is available), ugly but took so long to find out how to delete the correct days\n",
        "df_no_missing_days = df_including_na.copy().drop(df_including_na.index[[0,1, 389, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645]])\n",
        "\n",
        "# Interpolate 'valence', only 2 cases where all data is allright except for valence\n",
        "df_no_missing_days['circumplex.valence'] = df_no_missing_days['circumplex.valence'].interpolate(method='linear')\n",
        "\n",
        "# Interpolate 'activity' values, so note NO EXTRAPOLATION!\n",
        "df_no_missing_activity = df_no_missing_days.groupby('user_id').apply(lambda group: group.interpolate(method='linear', limit=10, limit_area= 'inside', order=1))\n",
        "\n",
        "# Delete rows that still exist, because these need to be extrapolated which seems not very realistic\n",
        "df_no_missing_activity = df_no_missing_activity.dropna()\n",
        "\n",
        "# Function that inserts a row with missing values for the days that are not in the dataset\n",
        "def fill_days(x): \n",
        "    missing_dates = pd.date_range(start = x.index.min(), end = x.index.max()).difference(x.index)\n",
        "    for missing_date in missing_dates:\n",
        "        x.loc[missing_date] = [np.nan] * x.shape[-1]\n",
        "        x = x.sort_values(by=\"datetime\")\n",
        "    return x\n",
        "\n",
        "# Insert missing days using the function above\n",
        "df_added_days = df_no_missing_activity.copy().reset_index(level=0).groupby('user_id').apply(fill_days)\n",
        "# Somehow this column is made so I delete it here\n",
        "df_added_days = df_added_days.drop(columns='user_id')\n",
        "\n",
        "# interpolate values for the inserted days\n",
        "df_final = df_added_days.groupby('user_id').apply(lambda group: group.interpolate(method='linear', limit=2))\n",
        "\n",
        "\n",
        "# print dataframe fully\n",
        "data_table.DataTable(df_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuinSr9-WWSo"
      },
      "source": [
        "## CREATE THE 5 DAILY BY SLIDING OVER THE DATA \n",
        "##          WITH A WINDOW OF SIZE 5 & COMPUTE DELTA'\n",
        "\n",
        "# Generate rolling mean and rolling sum with window = 5. Shift=1 since 'rolling'\n",
        "# includes current day\n",
        "\n",
        "df_aggregated_daily = df_final.copy()\n",
        "#print(df_aggregated_daily.head())\n",
        "df_avg_rolling_mean_5daily = df_aggregated_daily.reset_index(level=0).groupby('user_id').rolling(window=5).mean().shift(1) #.groupby('user_id')\n",
        "df_avg_rolling_sum_5daily = df_aggregated_daily.reset_index(level=0).groupby('user_id').rolling(window=5).sum().shift(1)\n",
        "# print(df_avg_rolling_sum_5daily.head(6))\n",
        "# print(df_avg_rolling_sum_5daily.head(6))\n",
        "\n",
        "## GROUP BY\n",
        "## REMOVE NA VALUES\n",
        "## CHANGE WINDOW TYPE\n",
        "\n",
        "# Take the sum for everything\n",
        "df_avg_5daily = df_avg_rolling_sum_5daily\n",
        "\n",
        "# Take the mean of relevant attributes (the mood-related attributes )\n",
        "df_avg_5daily[\"mood\"] = df_avg_rolling_mean_5daily[\"mood\"]\n",
        "df_avg_5daily[\"circumplex.arousal\"] = df_avg_rolling_mean_5daily[\"circumplex.arousal\"]\n",
        "df_avg_5daily[\"circumplex.valence\"] = df_avg_rolling_mean_5daily[\"circumplex.valence\"]\n",
        "df_avg_5daily[\"activity\"] = df_avg_rolling_mean_5daily[\"activity\"]\n",
        "\n",
        "# Rename columns\n",
        "df_avg_5daily = df_avg_5daily.add_prefix('AVG_')\n",
        "df_avg_5daily = df_avg_5daily.rename(columns = {'AVG_mood':'MEAN_mood', \n",
        "            'AVG_circumplex.arousal':'MEAN_circumplex.arousal', 'AVG_circumplex.valence':'MEAN_circumplex.valence', \n",
        "            'AVG_activity':'MEAN_activity',})\n",
        "\n",
        "## CALCULATE DELTA\n",
        "## Option 1: Take sum of absolute differences between rows:\n",
        "## Subtract each value with value of last row: take absolute difference\n",
        "df_aggregated_daily_differences = abs(df_aggregated_daily.groupby('user_id').diff()) # <- .groupby('user_id')\n",
        "df_5daily_delta = df_aggregated_daily_differences.reset_index(level=0).groupby('user_id').rolling(window=5).sum().shift(1)\n",
        "\n",
        "\n",
        "## Option 2: Take trend of last 5 datapoints\n",
        "# polyfit = lambda y: np.polyfit(range(len(y)), y, 1)[0] # 0 = a, 1 = b (y=ax+b). Take 1st order polynomial\n",
        "# df_5daily_delta = df_aggregated_daily.rolling(window=5).apply(polyfit).shift(1)\n",
        "\n",
        "#NOTE: BECAUSE OF DIFFERENCE THE VALUES NOW START FROM THE 7'TH ROW (NOT 6'TH)\n",
        "\n",
        "# Rename columns\n",
        "df_5daily_delta = df_5daily_delta.add_prefix('DELTA_')\n",
        "# Remove weekdays\n",
        "df_5daily_delta = df_5daily_delta.loc[:,'DELTA_activity':'DELTA_sms']\n",
        "\n",
        "# Join 5-daily-DELTA-df and 5-daily-avg-df\n",
        "df_avg_5daily = pd.concat([df_avg_5daily, df_5daily_delta], axis=1)\n",
        "\n",
        "# Add label to df: actual avg mood on that day\n",
        "\n",
        "df_avg_5daily['LABEL_mood'] = df_aggregated_daily.loc[:,'mood']\n",
        "\n",
        "#df_avg_5daily['Monday':'Sunday'] = df_aggregated_daily['Monday':'Sunday']\n",
        "# Add weekdays as a one hot encoding to the dataframe\n",
        "names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "for i, x in enumerate(names):\n",
        "    df_avg_5daily[x] = (df_avg_5daily.index.get_level_values(1).weekday == i).astype(int)\n",
        "\n",
        "# Drop all rows with na-values: this drops first 6 rows of every user\n",
        "df_avg_5daily = df_avg_5daily.dropna()\n",
        "\n",
        "## 1st 5 values communication: \n",
        "#print(df_aggregated_daily.iloc[0:5, 2])\n",
        "# user_id  datetime  \n",
        "# AS14.01  2014-03-21     6280.890\n",
        "#          2014-03-22     4962.918\n",
        "#          2014-03-23     5237.319\n",
        "#          2014-03-24     9270.629\n",
        "#          2014-03-25    10276.751\n",
        "# Name: appCat.communication, dtype: float64\n",
        "# Slope: \n",
        "#print( np.polyfit(range(len(df_aggregated_daily.iloc[0:5, 2].index)), df_aggregated_daily.iloc[0:5, 2], 1) )\n",
        "#1229.9432999999983\n",
        "# test = df_aggregated_daily.iloc[0:5, 2]\n",
        "# test = np.array(test)\n",
        "# print(test)\n",
        "# coef = np.polyfit(range(len(test)), test, 1)\n",
        "# print( coef )\n",
        "# plt.plot(test)\n",
        "# plt.plot([coef[0]*x + coef[1] for x in range(len(test))])\n",
        "# plt.show()\n",
        "\n",
        "#df_aggregated_daily.head()\n",
        "#df_aggregated_daily_differences.head(10)\n",
        "#df_5daily_delta.head(10)\n",
        "df_avg_5daily.head(n=10)\n",
        "#data_table.DataTable(df_avg_5daily)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jTGrVTkW2vr"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), scipy.stats.sem(a)\n",
        "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "    return m, m-h, m+h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ss73dr5DWc9"
      },
      "source": [
        "## Baseline\n",
        "The baseline for this project is the average mood per participant on the previous day. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UscogSUGDVkF"
      },
      "source": [
        "# by grouping the data per user and shifting the values 1 place we get the baseline model\n",
        "df_aggregated_daily_baseline = df_final.copy()\n",
        "df_aggregated_daily_baseline['predicted mood'] = df_aggregated_daily_baseline['mood'].groupby(\n",
        "    'user_id').shift(1)\n",
        "\n",
        "# No longer needed to create np arrays: pandas arrays are also accepted now\n",
        "# create np arrays for the labels and the predictons to compare these two\n",
        "labels = np.array(df_aggregated_daily_baseline['mood'])\n",
        "predictions = np.array(df_aggregated_daily_baseline['predicted mood'])\n",
        "print(\"Number of NaN predictions :\", np.count_nonzero(np.isnan(predictions)), \n",
        "      \"(a NaN value for every first observation per user)\")\n",
        "\n",
        "errors_bl = abs(predictions - labels)\n",
        "errors_bl = errors_bl[~np.isnan(errors_bl)]\n",
        "squared_errors_bl = np.power(errors_bl,2)\n",
        "mse=round(np.mean(np.power(errors_bl,2)), 2)\n",
        "print('Confidence Interval MSE (Baseline): ', mean_confidence_interval(squared_errors_bl))\n",
        "print('Confidence Interval MAE (Baseline): ', mean_confidence_interval(errors_bl)) \n",
        "\n",
        "# Dataframe with predictions and dates\n",
        "predictions_data = pd.DataFrame(data = {'date': range(len(predictions)), 'prediction': predictions})\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.rcParams['axes.facecolor'] = 'lightgrey'\n",
        "# Plot the actual values\n",
        "plt.plot(range(len(labels)), labels, 'b-', label = 'actual')\n",
        "# Plot the predicted values\n",
        "plt.plot(range(len(predictions)), predictions, 'ro', label = 'prediction')\n",
        "plt.xticks(rotation = '60'); \n",
        "plt.legend()\n",
        "# Graph labels\n",
        "plt.xlabel('Days in test set'); plt.ylabel('Mood'); plt.title('BASELINE Actual and Predicted Values. MSE = {}'.format(mse));\n",
        "\n",
        "# # print dataframe fully\n",
        "# data_table.DataTable(df_aggregated_daily_baseline)\n",
        "plt.savefig(\"baseline.pdf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DB23WAIbf35"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejiJLHtA8iaf"
      },
      "source": [
        "avg5day = True\n",
        "fulldata = False\n",
        "\n",
        "if avg5day :\n",
        "    # Labels are the mood values we want to predict\n",
        "    labels = df_avg_5daily['LABEL_mood']\n",
        "    # Features totally \n",
        "    if fulldata: # with all variables still in the dataframe\n",
        "        features = df_avg_5daily.drop(['MEAN_mood', 'MEAN_circumplex.arousal', \n",
        "                                    'MEAN_circumplex.valence', 'LABEL_mood',\n",
        "                                    'DELTA_mood', 'DELTA_circumplex.arousal',\n",
        "                                    'DELTA_circumplex.valence'], axis = 1)\n",
        "    else: # without variable pruned because they are < (max/4)\n",
        "        features = df_avg_5daily.drop(['MEAN_mood', 'MEAN_circumplex.arousal', \n",
        "                                    'MEAN_circumplex.valence', 'LABEL_mood',\n",
        "                                    'DELTA_mood', 'DELTA_circumplex.arousal',\n",
        "                                    'DELTA_circumplex.valence', \n",
        "                                    'AVG_appCat.finance', 'AVG_appCat.game',\n",
        "                                    'AVG_appCat.weather', 'DELTA_appCat.finance',\n",
        "                                    'DELTA_appCat.game', 'DELTA_appCat.weather',\n",
        "                                    'Monday', 'Tuesday', 'Wednesday','Thursday',\n",
        "                                    'Friday', 'Saturday', 'Sunday' ], axis = 1)\n",
        "                                \n",
        "\n",
        "                                \n",
        "else:\n",
        "    # Labels are the mood values we want to predict\n",
        "    labels = np.array(df_aggregated_daily['mood'])\n",
        "    # We want to predict mood so we have to take it out of the dataset to predict\n",
        "    ## FEATURES without the mood:\n",
        "    if originaldata:\n",
        "        features = df_aggregated_daily.drop(['mood', 'circumplex.arousal', 'circumplex.valence'], axis = 1)\n",
        "    else:\n",
        "        ## FEATURES without the mood + importance > 0.03:\n",
        "        features = df_aggregated_daily.drop(['mood', 'circumplex.arousal', 'circumplex.valence',\n",
        "                                             'appCat.finance', 'appCat.game', 'appCat.office',\n",
        "                                             'appCat.unknown', 'appCat.weather'], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "# Saving feature names for later use in the plot\n",
        "feature_list = list(features.columns)\n",
        "\n",
        "# No longer needed: pandas also accepted\n",
        "# Convert to numpy array\n",
        "features = np.array(features)\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    features, labels, test_size = 0.25, random_state = 42)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
        "rf.fit(train_features, train_labels)\n",
        "\n",
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(test_features)\n",
        "\n",
        "# Print out the mean squared error (MSE)\n",
        "errors_rf = abs(predictions - test_labels)\n",
        "errors_rf = errors_rf[~np.isnan(errors_rf)]\n",
        "squared_errors_rf = np.power(errors_rf,2)\n",
        "mse=round(np.mean(np.power(errors_rf,2)), 2)\n",
        "print('Confidence Interval MSE (Random Forest): ', mean_confidence_interval(squared_errors_rf))\n",
        "print('Confidence Interval MAE (Random Forest): ', mean_confidence_interval(errors_rf))\n",
        "\n",
        "# Get numerical feature importances\n",
        "importances = list(rf.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in \n",
        "                       zip(feature_list, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "print(\"Max importance: \", max(importances)/4)\n",
        "\n",
        "%matplotlib inline\n",
        "# Set the style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['axes.facecolor'] = 'lightgrey'\n",
        "if avg5day:\n",
        "    plt.rcParams[\"figure.figsize\"]=(10, 10) # Including DELTA / Weekdays\n",
        "# list of x locations for plotting\n",
        "x_values = list(range(len(importances)))\n",
        "# Make a bar chart\n",
        "# plt.bar(x_values, importances, orientation = 'vertical')\n",
        "plt.barh(list(reversed(feature_list)), list(reversed(importances)))\n",
        "# Tick labels for x axis\n",
        "#plt.xticks(x_values, feature_list, rotation='vertical')\n",
        "# Axis labels and title\n",
        "cutoff = max(importances)/4\n",
        "boolimp = importances < cutoff\n",
        "for i in range(len(boolimp)):\n",
        "    if boolimp[i]: \n",
        "        print(feature_list[i]) \n",
        "#print(feature_list[importances < cutoff])\n",
        "plt.vlines(x=cutoff, ymin=-1, ymax=len(feature_list), linestyles=\"dashed\", colors=\"red\", )\n",
        "plt.ylabel('Variable'); plt.title('Variable Importances for RF (with MSE test data = {})'.format(mse))\n",
        "plt.xlabel('Importance, cutoff = (max/4) ={}'.format(round(cutoff,3)))\n",
        " \n",
        "plt.savefig(\"variable_importances.pdf\", bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOoNbTfu6b5H"
      },
      "source": [
        "# Dataframe with predictions and dates\n",
        "predictions_data = pd.DataFrame(data = {'date': range(len(predictions)), 'prediction': predictions})\n",
        "# Plot the actual values\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.rcParams['axes.facecolor'] = 'lightgrey'\n",
        "plt.plot(range(len(predictions)), test_labels, 'b-', label = 'actual')\n",
        "# Plot the predicted values\n",
        "plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
        "plt.xticks(rotation = '60'); \n",
        "plt.legend()\n",
        "# Graph labels\n",
        "plt.xlabel('Days in test set'); plt.ylabel('Mood'); plt.title('RANDOM FOREST Actual and Predicted Values. MSE = {}'.format(mse));\n",
        "plt.savefig(\"random_forest.pdf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAAtUi4k44wU"
      },
      "source": [
        "## LSTM\n",
        "### many-to-one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFmIU-gP47cF"
      },
      "source": [
        "# function to transfrom daily data to lstm ready format\n",
        "\n",
        "def lstm_data_transform(data, num_steps=5):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop over the user's data set\n",
        "    users = data.index.unique(0)\n",
        "    for user in users:\n",
        "        u_data = data.filter(like=user, axis=0)       \n",
        "        y_data = u_data['mood']\n",
        "        x_data = u_data.drop(columns='mood')\n",
        "        for i in range(u_data.shape[0]):\n",
        "            # compute a new (sliding window) index\n",
        "            end_ix = i + num_steps\n",
        "            # if index is larger than the size of the dataset, we stop\n",
        "            if end_ix >= u_data.shape[0]:\n",
        "                break\n",
        "            # Get a sequence of data for x\n",
        "            seq_X = x_data.iloc[i:end_ix] # update for columns if changed!\n",
        "            # Get only the last element of the sequency for y\n",
        "            seq_y = y_data.iloc[end_ix]\n",
        "            # Append the list with sequencies\n",
        "            X.append(seq_X)\n",
        "            y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuxKY1MmlecG"
      },
      "source": [
        "# normalizes values\n",
        "\n",
        "def standardize(data):\n",
        "    mood = data['mood']\n",
        "    data_mean = data.mean(axis=0)\n",
        "    data_std = data.std(axis=0)\n",
        "    data = (data - data_mean) / data_std\n",
        "    data['mood'] = mood\n",
        "    return data\n",
        "\n",
        "def normalize(data):\n",
        "    mood = data['mood']\n",
        "    data = (data-data.min())/(data.max()-data.min())\n",
        "    data['mood'] = mood\n",
        "                             \n",
        "    return data\n",
        "#     x = data.values #returns a numpy array\n",
        "#     cols = data.columns\n",
        "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
        "#     x_scaled = min_max_scaler.fit_transform(x)\n",
        "#     df = pd.DataFrame(x_scaled)\n",
        "#     df.columns = cols\n",
        "#     return df\n",
        "\n",
        "#t_data = standardize(df_aggregated_daily.drop(columns=['appCat.finance','appCat.game','appCat.office','appCat.unknown','appCat.weather']))\n",
        "t_data = normalize(df_aggregated_daily.drop(columns=['appCat.finance','appCat.game','appCat.office','appCat.unknown','appCat.weather']))\n",
        "#t_data = normalize(df_aggregated_daily)\n",
        "\n",
        "# splits data into train and test data\n",
        "\n",
        "train_ind = int(0.75 * t_data.shape[0])\n",
        "train_data = t_data[:train_ind] #df_aggregated_daily\n",
        "test_data = t_data[train_ind:]\n",
        "\n",
        "# transforms it to be traing ready\n",
        "num_steps = 5\n",
        "# training set\n",
        "(x_train_transformed,\n",
        " y_train_transformed) = lstm_data_transform(train_data, num_steps=num_steps)\n",
        "assert x_train_transformed.shape[0] == y_train_transformed.shape[0]\n",
        "# test set\n",
        "(x_test_transformed,\n",
        " y_test_transformed) = lstm_data_transform(test_data, num_steps=num_steps)\n",
        "assert x_test_transformed.shape[0] == y_test_transformed.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhhz7KealzWP"
      },
      "source": [
        "# # compile model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(13, activation='tanh', input_shape=(num_steps, 13), return_sequences=False))\n",
        "model.add(keras.layers.Dense(units=13, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='linear'))\n",
        "adam = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=adam, loss='mse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmRLJes-mAlf"
      },
      "source": [
        "# fit model and predict\n",
        "model.fit(x_train_transformed, y_train_transformed, epochs=10)\n",
        "test_predict = model.predict(x_test_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHGFwHz7mBVm"
      },
      "source": [
        "# Dataframe with predictions and dates\n",
        "predictions_data = pd.DataFrame(data = {'date': range(len(test_predict)), 'prediction': test_predict.squeeze()})\n",
        "# Plot the actual values\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.rcParams['axes.facecolor'] = 'lightgrey'\n",
        "plt.plot(range(len(y_test_transformed)), y_test_transformed, 'b-', label = 'actual')\n",
        "# Plot the predicted values\n",
        "plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
        "plt.xticks(rotation = '60'); \n",
        "plt.legend()\n",
        "# Graph labels\n",
        "mse=round(mean_squared_error(test_predict, y_test_transformed), 2)\n",
        "plt.xlabel('Days in test set'); plt.ylabel('Mood'); plt.title('LSTM Actual and Predicted Values. MSE = {}'.format(mse))\n",
        "\n",
        "# Print out the mean squared error (MSE)\n",
        "errors_lstm = abs(predictions_data['prediction'] - y_test_transformed)\n",
        "errors_lstm = errors_lstm[~np.isnan(errors_lstm)]\n",
        "squared_errors_lstm = np.power(errors_lstm,2)\n",
        "mse=round(np.mean(np.power(errors_lstm,2)), 2)\n",
        "print('Confidence Interval MSE (LSTM): ', mean_confidence_interval(squared_errors_lstm))\n",
        "print('Confidence Interval MAE (LSTM): ', mean_confidence_interval(errors_lstm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPn85cq4egap"
      },
      "source": [
        "# Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBrPix5RbUs1"
      },
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "print(\"Random Forest vs Baseline\")\n",
        "print(scipy.stats.mannwhitneyu(squared_errors_rf, squared_errors_bl))\n",
        "print(\"\\nLSTM vs Baseline\")\n",
        "print(scipy.stats.mannwhitneyu(squared_errors_lstm, squared_errors_bl))\n",
        "print(\"\\nLSTM vs Random Forest\")\n",
        "print(scipy.stats.mannwhitneyu(squared_errors_lstm, squared_errors_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifeIfCrFKgh"
      },
      "source": [
        "## LSTM\n",
        "### many-to-many"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12qEjH0AID44"
      },
      "source": [
        "# splitting \n",
        "train_ind = int(0.75 * t_data.shape[0])\n",
        "x_train = t_data[:train_ind].drop(columns='mood')\n",
        "x_test = t_data[train_ind:].drop(columns='mood')\n",
        "y_train = t_data['mood'][:train_ind].values.reshape(-1, 1)\n",
        "y_test = t_data['mood'][train_ind:].values.reshape(-1, 1)\n",
        "print(y_test.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8L4B9g5Hjf4"
      },
      "source": [
        "# scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "x_train_sc = scaler_x.fit_transform(x_train)\n",
        "x_test_sc = scaler_x.transform(x_test)\n",
        "y_train_sc = scaler_y.fit_transform(y_train)\n",
        "y_test_sc = scaler_y.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC1ZkRzGIQIh"
      },
      "source": [
        "# reshape\n",
        "num_steps = 3\n",
        "# training set\n",
        "x_train_shaped = np.reshape(x_train_sc[:864], newshape=(-1, num_steps, 18))\n",
        "y_train_shaped = np.reshape(y_train_sc[:864], newshape=(-1, num_steps, 1))\n",
        "assert x_train_shaped.shape[0] == y_train_shaped.shape[0]\n",
        "# test set\n",
        "x_test_shaped = np.reshape(x_test_sc[:90], newshape=(-1, num_steps, 18))\n",
        "y_test_shaped = np.reshape(y_test_sc[:90], newshape=(-1, num_steps, 1))\n",
        "assert x_test_shaped.shape[0] == y_test_shaped.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHsJahrXITdP"
      },
      "source": [
        "# compile model\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(18, activation='tanh', input_shape=(num_steps, 18), return_sequences=True))\n",
        "model.add(keras.layers.Dense(units=18, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='linear'))\n",
        "adam = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=adam, loss='mse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUXFuj7CIXz4"
      },
      "source": [
        "model.fit(x_train_shaped, y_train_shaped, epochs=30)\n",
        "test_predict = model.predict(x_test_shaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GlA8LStLX1F"
      },
      "source": [
        "print(test_predict.reshape(90,1).squeeze(), y_test_shaped.shape)\n",
        "print('Mean Squared Error over test data (LSTM):', \n",
        "      round(mean_squared_error(test_predict.squeeze(), y_test_shaped.squeeze()), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIc0lZyBIbpo"
      },
      "source": [
        "# Dataframe with predictions and dates\n",
        "predictions_data = pd.DataFrame(data = {'date': range(len(test_predict.reshape(90,1).squeeze())), 'prediction': test_predict.reshape(90,1).squeeze()})\n",
        "# Plot the actual values\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.rcParams['axes.facecolor'] = 'lightgrey'\n",
        "plt.plot(range(len(y_test_transformed.reshape(257,1).squeeze())), y_test_transformed.reshape(257,1).squeeze(), 'b-', label = 'actual')\n",
        "# Plot the predicted values\n",
        "plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
        "plt.xticks(rotation = '60'); \n",
        "plt.legend()\n",
        "# Graph labels\n",
        "mse= round(mean_squared_error(test_predict, y_test_transformed), 2)\n",
        "plt.xlabel('Days in test set'); plt.ylabel('Mood'); plt.title('LSTM Actual and Predicted Values. MSE = {}'.format(mse));\n",
        "\n",
        "print('Mean Squared Error over test data (LSTM):', mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhFqxZ0pN7Ob"
      },
      "source": [
        "# RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKezUUctN-S7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLZ-nEC5Q6zn"
      },
      "source": [
        "\n",
        "# Simple: \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "optimizer = keras.optimizers.Adam(lr=0.005)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history = model.fit(x_train_transformed, y_train_transformed, epochs=20,\n",
        "                    validation_data=(x_test_transformed, y_test_transformed))\n",
        "model.evaluate(x_test_transformed, y_test_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIOmbIcO0PT"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    features, labels, test_size = 0.25, random_state = 42)\n",
        "\n",
        "# Simple: \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "optimizer = keras.optimizers.Adam(lr=0.005)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjX01xBB7sxA"
      },
      "source": [
        "Temporal Approach -> I thought LSTM could be nice? Or a time series approach but I do not know much about those (Autoregressive integrated moving average is the proposed time series model, but this only uses the mood as a variable I think)\n",
        "\n",
        "Things to try out: random forest, ARIMA, RNNs, prophet if the dataset is univariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqdgNJsEj52"
      },
      "source": [
        "**Aggregating Data (not based on literature sadly):**\n",
        "\n",
        "\n",
        "1.   Make everything hourly (ex : # of pickups within 1 app-domain, total minutes of usage within 1 app-domain, both in 1 hour). Some attributes are hourly by default (such as activity) so we will want to make everything hourly\n",
        "2.   Make everything daily (THINK ABOUT: maybe also account for mornings/afternoons/evenings/night. Will that make a difference?). Same examples as above. \n",
        "3.   Make everything 5-daily. IMPORTANT: Also measure the DELTA for every attribute. How much did the behaviour/measurements change over the days(/mornings/afternoons/evenings/nights)?\n",
        "\n",
        "=> END RESULT: We end up with \n",
        "*   average numbers over 5 day-periods (same example as 1st )\n",
        "*   DELTA of attributes over days. This might be even more important than averages etc as in point above\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knNVUe09HzbK"
      },
      "source": [
        "**Attributes for aggregating History**\n",
        "\n",
        "*   Mood/Arousal/Valence: avg per day/part-of-day (don't include 0's)\n",
        "*   Activity: \"\n",
        "*   Call/SMS: \"\n",
        "*  Screen/All-apps: # of pickups, total time (or average already)\n",
        "\n",
        "ALL: DELTA-parameter which summarizes how much the behaviour/mood changed over days (positive/more or negative/less)\n",
        "\n",
        "https://www.researchgate.net/publication/310598732_How_to_Predict_Mood_Delving_into_Features_of_Smartphone-_Based_Data (link on how previous VU researcers published a paper about this)\n",
        "\n",
        "---\n",
        "Also include day of the week as attribute\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4XFw1yICHFA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztUVzRmIfQN9"
      },
      "source": [
        "TODO: (11-4)\n",
        "\n",
        "\n",
        "*   In 'df_avg_5daily' discriminate between users: probably use 'groupby' (now the rolling window just continues)\n",
        "*   join data data is NA: now we drop everything -delete NA when user starts later. Fill in (interpolate) randomly missing days (such as 6th of May)\n",
        "*   DELTA: try using np.polyfit to fit the trend ( https://www.emilkhatib.com/analyzing-trends-in-data-with-pandas/ )\n",
        "\n",
        "*   REPORT: Compare attributes: including mood/arousal/valence, excluding mood/arousal/valence, excluding DELTA-mood/arousal/valence (mainly to show how close our MSE is without all mood stuff\n",
        "\n",
        "* Specify potential window function for rolling : https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows \n",
        "\n"
      ]
    }
  ]
}