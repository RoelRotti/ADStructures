{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Intro2NLP_pipelines_with_spacy.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoelRotti/ADStructures/blob/master/Intro2NLP_pipelines_with_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiB2tCzjt2pd"
      },
      "source": [
        "# Introduction to NLP\n",
        "\n",
        "# Lab 1: Pipelines with spaCy\n",
        "\n",
        "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
        "This notebook is based on an earlier version developed by Piek Vossen and Selene Baez. The [original version](https://github.com/cltl/ma-hlt-labs/blame/master/lab1.toolkits/Lab1.3-introduction-to-spaCy.ipynb) is more detailed and might be helpful if you have limited programming experience.\n",
        "\n",
        "[SpaCy](https://spacy.io/) combines multiple natural language processing analyses in a single Pythin package: it takes a raw document and can perform tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER). The advantage of spaCy is that it is really fast, and it has a good accuracy. In addition, it currently supports multiple languages, among which: English, German, Spanish, Portuguese, French, Italian and Dutch. Other popular Python packages are [nltk](https://www.nltk.org/) and [stanza](https://stanfordnlp.github.io/stanza/). \n",
        "\n",
        "In this notebook, we will show you the basic usage of spaCy. Please additionally check the [user guides](https://spacy.io/usage/linguistic-features) and the documentation of the [models](https://spacy.io/models) for details.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNmvfNYt2pg"
      },
      "source": [
        "## Installing and loading spaCy\n",
        "\n",
        "To install spaCy, check out the instructions [here](https://spacy.io/usage). On this page, it is explained exactly how to install spaCy for your operating system, package manager and desired languages. Simply run the suggested commands in your terminal ([Anaconda Prompt](https://docs.anaconda.com/anaconda/user-guide/getting-started/) or cmd). In this notebook, we are going to download the English language resources. The standard download command from the command line is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06NK3uRMt2ph",
        "outputId": "c9eb74fb-1729-457f-c9ad-1ea94a69e880"
      },
      "source": [
        "%%bash\n",
        "python -m venv .env           # will create directory .env\n",
        "source .env/bin/activate      # activate virtualenv\n",
        "pip install spacy             # install stuff\n",
        "python -m spacy download en   # this should now work as expected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Error: Command '['/content/.env/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "bash: line 2: .env/bin/activate: No such file or directory\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJK8KLfXt2pi"
      },
      "source": [
        "Now, let's first load spaCy in the notebook and check if we can load the English language resources. We import the spaCy module and load the English tokenizer, tagger, parser, NER, and word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX9qSWxjt2pi"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# This loads a small English model trained on web data.\n",
        "# For other models and languages check: https://spacy.io/models\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv8NI1kGt2pi"
      },
      "source": [
        "## Using spaCy\n",
        "\n",
        "If you succesfully loaded the English model (or another language), you now created the spaCy object 'nlp'. You can use it to process text through a defined pipeline of modules and store the result as a value for another variable for accessing it. The results is another spaCy object of the type 'Doc' which gives you access to all the different analyses of the pipeline through different functions. In a Doc object you can access tokens, their lemmas, their PoS, sentences, chunks, named entities, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAS6GpGpt2pj"
      },
      "source": [
        "test_input = \"I have an awesome cat in the U.S.A. It's sitting on the mat that I bought accordingly.\"\n",
        "# Let's run the NLP pipeline on our test input\n",
        "doc = nlp(test_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrR0k462t2pj"
      },
      "source": [
        "### 1. Tokenization\n",
        "The basic unit in NLP is usually the token. Let's examine how spaCy tokenizes the input. \n",
        "Note that punctuation is treated as a separate token and check how \"It's\" is tokenized. **Try a few other test inputs to better understand the concept of a token.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "522fxhojt2pk",
        "outputId": "6ec59c56-764d-42e0-f657-a8eb4d0c2cfa"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.i, token, token.idx)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 I 0\n",
            "1 have 2\n",
            "2 an 7\n",
            "3 awesome 10\n",
            "4 cat 18\n",
            "5 in 22\n",
            "6 the 25\n",
            "7 U.S.A. 29\n",
            "8 It 36\n",
            "9 's 38\n",
            "10 sitting 41\n",
            "11 on 49\n",
            "12 the 52\n",
            "13 mat 56\n",
            "14 that 60\n",
            "15 I 65\n",
            "16 bought 67\n",
            "17 accordingly 74\n",
            "18 . 85\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5VCuaOft2pk"
      },
      "source": [
        "In addition, spaCy provides sentence segmentation by grouping tokens together. **Try different test inputs to analyze the quality of the sentence segmentation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oca2znZct2pk",
        "outputId": "491c6c47-d577-4cea-d5fc-e71f8d2ed121"
      },
      "source": [
        "sentences = doc.sents\n",
        "for sentence in sentences:\n",
        "    print()\n",
        "    print(sentence)\n",
        "    for token in sentence:\n",
        "        print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "I have an awesome cat in the U.S.A.\n",
            "I\n",
            "have\n",
            "an\n",
            "awesome\n",
            "cat\n",
            "in\n",
            "the\n",
            "U.S.A.\n",
            "\n",
            "It's sitting on the mat that I bought accordingly.\n",
            "It\n",
            "'s\n",
            "sitting\n",
            "on\n",
            "the\n",
            "mat\n",
            "that\n",
            "I\n",
            "bought\n",
            "accordingly\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u44slfwZt2pk"
      },
      "source": [
        "### 2. Lemmatization\n",
        "The Token object contains much more information than just the String representing the word. For example, you can access the lemma of each token. Note, that spaCy delivers a good accuracy, but it does make mistakes. **Make sure you understand the difference between a token and a lemma. Try out a few tricky cases as test input to analyze the quality of the lemmatizer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzR63V0ft2pl",
        "outputId": "fd478d0a-1205-4e6c-e61b-2b5e9a55855b"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I -PRON-\n",
            "have have\n",
            "an an\n",
            "awesome awesome\n",
            "cat cat\n",
            "in in\n",
            "the the\n",
            "U.S.A. U.S.A.\n",
            "It -PRON-\n",
            "'s be\n",
            "sitting sit\n",
            "on on\n",
            "the the\n",
            "mat mat\n",
            "that that\n",
            "I -PRON-\n",
            "bought buy\n",
            "accordingly accordingly\n",
            ". .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWl3S3cGt2pl"
      },
      "source": [
        "### 3. POS-Tagging\n",
        "A part-of-speech tagger assigns a word class to each token. The number of word classes depends on the tagset that the model uses. The most simplistic tags are the [universal POS-tags](https://universaldependencies.org/u/pos/all.html). Most models use more complex tagsets, but they also provide a mapping into the universal POS tags. SpaCy provides both: \n",
        "\n",
        "* the attribute **pos_** returns the universal part-of-speech tag\n",
        "* the attribute **tag_** provides a more finegrained tag\n",
        "\n",
        "The English model uses the [Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMCq_MOot2pl",
        "outputId": "02674957-73c8-49e7-c0be-6a9b0126fc25"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I PRON PRP\n",
            "have AUX VBP\n",
            "an DET DT\n",
            "awesome ADJ JJ\n",
            "cat NOUN NN\n",
            "in ADP IN\n",
            "the DET DT\n",
            "U.S.A. PROPN NNP\n",
            "It PRON PRP\n",
            "'s AUX VBZ\n",
            "sitting VERB VBG\n",
            "on ADP IN\n",
            "the DET DT\n",
            "mat NOUN NN\n",
            "that DET WDT\n",
            "I PRON PRP\n",
            "bought VERB VBD\n",
            "accordingly ADV RB\n",
            ". PUNCT .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaxCyOVit2pm"
      },
      "source": [
        "Make sure you understand the different tag labels. SpaCy provides a short explanation, but you also need to check the documentation and the reading material. **Find examples for words that can be assigned different POS-tags depending on the context. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AX0fjBXRt2pm",
        "outputId": "12461119-be2f-4000-b32d-193bd2cdff1d"
      },
      "source": [
        "spacy.explain(\"VBD\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'verb, past tense'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3IsAgQVt2pm"
      },
      "source": [
        "The `Token` objects have many more useful methods and attributes. List them using the Python function `dir()`. You can find more detailed information about the token methods and attributes in the [documentation](https://spacy.io/api/token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzUf7q1t2pm"
      },
      "source": [
        "first_token = doc[0]\n",
        "dir(first_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNX04EU3t2pn"
      },
      "source": [
        "Note that the attributes without `_` return numerical values which spaCy uses internally. Variants with `_` provide the human readable rendering of the value in unicode. **Explore some of the attributes and test them for different tokens.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLvHQSYQt2pn",
        "outputId": "f7b349d3-2cba-4ac8-fd03-46555e4e05d7"
      },
      "source": [
        "for i in range(5):\n",
        "    first_token = doc[i]\n",
        "    print(first_token.text, first_token.shape_, first_token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I X PRP\n",
            "have xxxx VBP\n",
            "an xx DT\n",
            "awesome xxxx JJ\n",
            "cat xxx NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_-NCSLwt2pn"
      },
      "source": [
        "### 4. Named Entity Recognition\n",
        "\"A named entity is a “real-world object” that is assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\" [[spaCy documentation]](https://spacy.io/usage/linguistic-features#named-entities)\n",
        "\n",
        "Explore the named entities in the example below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJnYriUit2po"
      },
      "source": [
        "text = \"But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8k0NaZ-t2po",
        "outputId": "67b4dae4-2846-42a2-ce18-7f1c95ab9f71"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google ORG\n",
            "Apple ORG\n",
            "Siri PRODUCT\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo PRODUCT\n",
            "Dot PRODUCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ZaANfsK2t2po",
        "outputId": "fc8967cc-ea9c-4e85-d235-c8d81e47206c"
      },
      "source": [
        "# Displacy provides nice visualizations of spaCy annotations https://spacy.io/usage/visualizers\n",
        "from spacy import displacy\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is starting from behind. The company made a late push into hardware, and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "’s \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Siri\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              ", available on iPhones, and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Amazon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "’s \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alexa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " software, which runs on its \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Echo\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dot\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " devices, have clear leads in consumer adoption.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd46q0hzt2pp"
      },
      "source": [
        "The English model is trained on a dataset called *OntoNotes* (version 5). **How many different named entity types are annotated in this dataset? Have a look at the [documentation](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIMS3alLt2pp"
      },
      "source": [
        "### 5. Calculating frequencies\n",
        "A common analysis step for language corpora is the extraction of frequency statistics. We provide an example to extract token frequencies, but you can also calculate frequencies over lemmas, n-grams, POS-labels, ...\n",
        "\n",
        "We calculate the statistics over a single input in this example. Usually, you would calculate them over all documents in a dataset. **How do you need to adjust the code to achieve this?** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tEhkZMlt2pp",
        "outputId": "75b522f5-4d01-4be8-b858-e5ac1ca48764"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Our test input is the first paragraph of https://spacy.io/usage/linguistic-features\n",
        "test_input = \"Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information. That’s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations.\"\n",
        "# Let's run the NLP pipeline on our test input\n",
        "doc = nlp(test_input)\n",
        "\n",
        "word_frequencies = Counter()\n",
        "\n",
        "for sentence in doc.sents:\n",
        "    words = []\n",
        "    for token in sentence: \n",
        "        # Let's filter out punctuation\n",
        "        if not token.is_punct:\n",
        "            words.append(token.text)\n",
        "    word_frequencies.update(words)\n",
        "    \n",
        "print(word_frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'to': 5, '’s': 4, 'raw': 3, 'text': 3, 'words': 3, 'it': 3, 'different': 3, 'in': 3, 'a': 3, 'is': 2, 'difficult': 2, 'and': 2, 'that': 2, 'completely': 2, 'mean': 2, 'the': 2, 'same': 2, 'can': 2, 'useful': 2, 'Processing': 1, 'intelligently': 1, 'most': 1, 'are': 1, 'rare': 1, 'common': 1, 'for': 1, 'look': 1, 'almost': 1, 'thing': 1, 'The': 1, 'order': 1, 'something': 1, 'Even': 1, 'splitting': 1, 'into': 1, 'word': 1, 'like': 1, 'units': 1, 'be': 1, 'many': 1, 'languages': 1, 'While': 1, 'possible': 1, 'solve': 1, 'some': 1, 'problems': 1, 'starting': 1, 'from': 1, 'only': 1, 'characters': 1, 'usually': 1, 'better': 1, 'use': 1, 'linguistic': 1, 'knowledge': 1, 'add': 1, 'information': 1, 'That': 1, 'exactly': 1, 'what': 1, 'spaCy': 1, 'designed': 1, 'do': 1, 'you': 1, 'put': 1, 'get': 1, 'back': 1, 'Doc': 1, 'object': 1, 'comes': 1, 'with': 1, 'variety': 1, 'of': 1, 'annotations': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urfEEBP5t2pq",
        "outputId": "7d392222-5159-4018-a491-50473b585622"
      },
      "source": [
        "num_tokens = len(doc)\n",
        "num_words = sum(word_frequencies.values())\n",
        "num_types = len(word_frequencies.keys())\n",
        "\n",
        "print(num_tokens, num_words, num_types)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117 105 74\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}